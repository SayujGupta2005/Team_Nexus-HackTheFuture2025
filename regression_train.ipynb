{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkqXXADHOMcg",
        "outputId": "fe4c9190-3fda-4138-d341-21af8fddc19d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIEG-eUGTccQ",
        "outputId": "61a771d6-a953-477b-a209-54df38faee08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Unweighted Metrics ===\n",
            "Train: MAE=629.1631, MAPE=15.0724, RMSE=874.5580, MSE=764851.6489, R2=0.8729\n",
            "Test: MAE=1026.5303, MAPE=23.3578, RMSE=1413.2334, MSE=1997228.6665, R2=0.6682\n",
            "\n",
            "=== Weighted Metrics ===\n",
            "Train Weighted: MAE=2546.3126, MAPE=15.0724, RMSE=3625.8289, MSE=13146635.1171, R2=0.8893\n",
            "Test Weighted: MAE=4123.7400, MAPE=23.3578, RMSE=5978.6455, MSE=35744202.2905, R2=0.7002\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from category_encoders import TargetEncoder\n",
        "from xgboost import XGBRegressor\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === Load Data ===\n",
        "train_df = pd.read_csv('data\\\\final_train_v3.csv')\n",
        "test_df  = pd.read_csv('data\\\\final_train_v3.csv')\n",
        "\n",
        "upper = train_df['Total_Expense'].quantile(0.95)\n",
        "for df in [train_df, test_df]:\n",
        "    df.drop(df[(df['Total_Expense'] > upper)].index, inplace=True)\n",
        "\n",
        "train_df = train_df[train_df.head_gender != 3].drop(columns=['HH_ID'])\n",
        "test_df  = test_df[test_df.head_gender != 3].drop(columns=['HH_ID'])\n",
        "\n",
        "train_weights = train_df.household_size\n",
        "test_weights  = test_df.household_size\n",
        "\n",
        "# === Column Lists ===\n",
        "binary_cols = ['Sector','mobile','head_gender','internet_use']\n",
        "high_card_cols = ['State','NSS_region','District','household_type','head_religion','head_social','profession','industry','head_education']\n",
        "int_cols = ['household_size','head_age','head_education_years','Is_couple']\n",
        "float_cols = [c for c in train_df.columns if c not in binary_cols + high_card_cols + int_cols + ['Total_Expense']]\n",
        "\n",
        "# === Binary Encoding ===\n",
        "for col in binary_cols:\n",
        "    uniques = pd.concat([train_df[col], test_df[col]]).dropna().unique()\n",
        "    mapping = {uniques[0]:0, uniques[1]:1}\n",
        "    train_df[col] = train_df[col].map(mapping).astype(int)\n",
        "    test_df[col]  = test_df[col].map(mapping).astype(int)\n",
        "\n",
        "# === Target Encoding ===\n",
        "te = TargetEncoder(cols=high_card_cols)\n",
        "y_train = train_df.pop('Total_Expense')\n",
        "train_df = te.fit_transform(train_df, y_train)\n",
        "y_test = test_df.pop('Total_Expense')\n",
        "test_df = te.transform(test_df)\n",
        "\n",
        "# === Scaling ===\n",
        "scaler = StandardScaler()\n",
        "train_df[int_cols + float_cols] = scaler.fit_transform(train_df[int_cols + float_cols])\n",
        "test_df[int_cols + float_cols]  = scaler.transform(test_df[int_cols + float_cols])\n",
        "\n",
        "X_train, X_test = train_df, test_df\n",
        "\n",
        "# === Train XGB with Best Hyperparameters ===\n",
        "best_params = {\n",
        "    'n_estimators': 750,\n",
        "    'max_depth': 10,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 1.0,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42,\n",
        "    'objective': 'reg:squarederror'\n",
        "}\n",
        "\n",
        "model = XGBRegressor(**best_params)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test  = model.predict(X_test)\n",
        "\n",
        "# === Evaluation Function ===\n",
        "def eval_metrics(y_true, y_pred, weights=None):\n",
        "    if weights is not None:\n",
        "        y_true, y_pred = y_true * weights, y_pred * weights\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, np.nan, y_true))) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mae, mape, rmse, mse, r2\n",
        "\n",
        "metrics_names = ['MAE','MAPE','RMSE','MSE','R2']\n",
        "\n",
        "print(\"\\n=== Unweighted Metrics ===\")\n",
        "for label, y_t, y_p in [('Train', y_train, y_pred_train), ('Test', y_test, y_pred_test)]:\n",
        "    vals = eval_metrics(y_t, y_p)\n",
        "    print(f\"{label}: \" + \", \".join(f\"{n}={v:.4f}\" for n, v in zip(metrics_names, vals)))\n",
        "\n",
        "print(\"\\n=== Weighted Metrics ===\")\n",
        "for label, y_t, y_p, w in [\n",
        "    ('Train Weighted', y_train, y_pred_train, train_weights),\n",
        "    ('Test Weighted',  y_test,  y_pred_test,  test_weights)\n",
        "]:\n",
        "    vals = eval_metrics(y_t, y_p, w)\n",
        "    print(f\"{label}: \" + \", \".join(f\"{n}={v:.4f}\" for n, v in zip(metrics_names, vals)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLR8EirpeEqV",
        "outputId": "9fac5122-a0a1-47c4-8767-ce39671e5586"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to disk\n",
        "joblib.dump(model, 'model\\\\xgb_regressor_model_below95.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZHwmn8TS3Mu",
        "outputId": "ed76eafb-e05e-4a1d-89dc-6686708317a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "✅ Best XGB Params: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100, 'subsample': 1.0}\n",
            "\n",
            "=== Unweighted Metrics ===\n",
            "Train: MAE=4577.4831, MAPE=21.7988, RMSE=7772.6244, MSE=60413689.7613, R2=0.3683\n",
            "Test: MAE=4779.1203, MAPE=22.4898, RMSE=8180.0686, MSE=66913521.7936, R2=0.1488\n",
            "\n",
            "=== Weighted Metrics ===\n",
            "Train Weighted: MAE=9799.6516, MAPE=21.7988, RMSE=18926.8572, MSE=358225922.0112, R2=0.6617\n",
            "Test Weighted: MAE=10403.6411, MAPE=22.4898, RMSE=20659.9468, MSE=426833401.0541, R2=0.5385\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from category_encoders import TargetEncoder\n",
        "from xgboost import XGBRegressor\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === Load Data ===\n",
        "train_df = pd.read_csv('data\\\\final_train_v3.csv')\n",
        "test_df  = pd.read_csv('data\\\\final_train_v3.csv')\n",
        "\n",
        "upper = train_df['Total_Expense'].quantile(0.95)\n",
        "for df in [train_df, test_df]:\n",
        "    df.drop(df[(df['Total_Expense'] < upper)].index, inplace=True)\n",
        "\n",
        "train_df = train_df[train_df.head_gender != 3].drop(columns=['HH_ID'])\n",
        "test_df  = test_df[test_df.head_gender != 3].drop(columns=['HH_ID'])\n",
        "\n",
        "train_weights = train_df.household_size\n",
        "test_weights  = test_df.household_size\n",
        "\n",
        "# === Column Lists ===\n",
        "binary_cols = ['Sector','mobile','head_gender','internet_use']\n",
        "high_card_cols = ['State','NSS_region','District','household_type','head_religion','head_social','profession','industry','head_education']\n",
        "int_cols = ['household_size','head_age','head_education_years','Is_couple']\n",
        "float_cols = [c for c in train_df.columns if c not in binary_cols + high_card_cols + int_cols + ['Total_Expense']]\n",
        "\n",
        "# === Binary Encoding ===\n",
        "for col in binary_cols:\n",
        "    uniques = pd.concat([train_df[col], test_df[col]]).dropna().unique()\n",
        "    mapping = {uniques[0]:0, uniques[1]:1}\n",
        "    train_df[col] = train_df[col].map(mapping).astype(int)\n",
        "    test_df[col]  = test_df[col].map(mapping).astype(int)\n",
        "\n",
        "# === Target Encoding ===\n",
        "te = TargetEncoder(cols=high_card_cols)\n",
        "y_train = train_df.pop('Total_Expense')\n",
        "train_df = te.fit_transform(train_df, y_train)\n",
        "y_test = test_df.pop('Total_Expense')\n",
        "test_df = te.transform(test_df)\n",
        "\n",
        "# === Scaling ===\n",
        "scaler = StandardScaler()\n",
        "train_df[int_cols + float_cols] = scaler.fit_transform(train_df[int_cols + float_cols])\n",
        "test_df[int_cols + float_cols]  = scaler.transform(test_df[int_cols + float_cols])\n",
        "\n",
        "X_train, X_test = train_df, test_df\n",
        "\n",
        "# === Grid Search ===\n",
        "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 250, 500],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.7, 0.85, 1.0],\n",
        "    'colsample_bytree': [0.5, 0.7, 0.85]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"✅ Best XGB Params:\", grid.best_params_)\n",
        "model = grid.best_estimator_\n",
        "\n",
        "# Retrain on full training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test  = model.predict(X_test)\n",
        "\n",
        "# === Evaluation Function ===\n",
        "def eval_metrics(y_true, y_pred, weights=None):\n",
        "    if weights is not None:\n",
        "        y_true, y_pred = y_true * weights, y_pred * weights\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, np.nan, y_true))) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mae, mape, rmse, mse, r2\n",
        "\n",
        "metrics_names = ['MAE','MAPE','RMSE','MSE','R2']\n",
        "\n",
        "print(\"\\n=== Unweighted Metrics ===\")\n",
        "for label, y_t, y_p in [('Train', y_train, y_pred_train), ('Test', y_test, y_pred_test)]:\n",
        "    vals = eval_metrics(y_t, y_p)\n",
        "    print(f\"{label}: \" + \", \".join(f\"{n}={v:.4f}\" for n, v in zip(metrics_names, vals)))\n",
        "\n",
        "print(\"\\n=== Weighted Metrics ===\")\n",
        "for label, y_t, y_p, w in [\n",
        "    ('Train Weighted', y_train, y_pred_train, train_weights),\n",
        "    ('Test Weighted',  y_test,  y_pred_test,  test_weights)\n",
        "]:\n",
        "    vals = eval_metrics(y_t, y_p, w)\n",
        "    print(f\"{label}: \" + \", \".join(f\"{n}={v:.4f}\" for n, v in zip(metrics_names, vals)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bACYQSDyS-Lv",
        "outputId": "00ae75e6-101b-45c4-837b-74725cf23781"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to disk\n",
        "joblib.dump(model, 'model\\\\xgb_regressor_model_above95.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQqM0hd8cMpz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
